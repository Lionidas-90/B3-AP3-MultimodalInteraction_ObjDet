{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection  - YOLO & OWL-ViT\n",
    "This tutorial demonstrates how to use YOLO (You Only Look Once) from the [Ultralytics](https://github.com/ultralytics/yolov5) library for object detection. It includes steps for:\n",
    "\n",
    "- Running object detection inference on images/videos\n",
    "- Fine-tuning YOLO for custom datasets\n",
    "- Comparing YOLO with OWl-VIT for zero-shot learning.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perform Object Detection Inference\n",
    "First thing We'll use YOLOv8 from Ultralyics for object detection on a sample image.\n",
    "We aim to utilize the pre-trained YOLOv8 model to detect objects in a sample image. This involves loading the model, providing an image for input, and interpreting the model's predictions.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Inference**: The process of using a trained model to make predictions on new data.\n",
    "- **YOLOv8**: A state-of-the-art version of the YOLO (You Only Look Once) architecture, known for its speed and accuracy in object detection tasks.\n",
    "\n",
    "**Steps:**\n",
    "1. Load the YOLOv8 model using the Ultralytics library.\n",
    "2. Perform inference on a sample image to detect objects.\n",
    "3. Visualize the results, including bounding boxes and class labels.\n",
    "\n",
    "**Support Material:**\n",
    "- https://docs.ultralytics.com/models/yolov8/\n",
    "- https://docs.ultralytics.com/tasks/detect/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import YOLO and load a pre-trained model\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load the YOLOv8 pre-trained model\n",
    "model = YOLO('yolov8n.pt')  # nano model for quick inference\n",
    "\n",
    "# Run inference on a sample image\n",
    "\n",
    "results = model('images/street_scene.jpg', save = True)  # Displays image with detections\n",
    "\n",
    "for result in results:\n",
    "    print(result.boxes)  # Boxes object for bounding box outputs\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fine-Tuning YOLO on Custom Dataset\n",
    "Fine-tuning YOLO requires a dataset formatted in the YOLO format. We'll use a small public dataset for demonstration.\n",
    "We will adapt the pre-trained YOLO model to a custom dataset. This process, known as fine-tuning, enables YOLO to specialize in detecting specific objects not included in its original training.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Fine-tuning**: Adapting a pre-trained model to new data by continuing the training process.\n",
    "- **Custom Dataset**: A dataset that contains specific objects relevant to a new application, different from those YOLO was trained on (e.g. https://docs.ultralytics.com/datasets/detect/signature/.) Does it work? yes, no? why not? what can you do?\n",
    "\n",
    "**Steps:**\n",
    "1. Prepare the custom dataset by organizing images and labels in the required format.\n",
    "2. Configure the YOLO training pipeline.\n",
    "3. Train the model and evaluate its performance.\n",
    "\n",
    "**Support Material:** \n",
    "- https://docs.ultralytics.com/modes/train/\n",
    "- https://docs.ultralytics.com/modes/val/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample dataset (e.g., Signature)\n",
    "!wget -q https://github.com/ultralytics/assets/releases/download/v0.0.0/signature.zip\n",
    "!unzip -oq signature.zip -d ./datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train YOLO on the dataset\n",
    "results = model.train(data='./datasets/signature.yaml', epochs=10, imgsz=640, batch=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /workspaces/B3-AP3-MultimodalInteraction_ObjDet/images/example_signature.jpg: 640x480 (no detections), 132.7ms\n",
      "Speed: 3.3ms preprocess, 132.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"runs/detect/train2/weights/best.pt\")  # load a custom model, check the path depending on your output before!! # Hier das beste training einf√ºgen\n",
    "\n",
    "# Predict with the model\n",
    "results = model.predict(\"images/example_signature.jpg\", conf=0.75) #check params if you need to improve detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Zero-Shot Learning with OWL-ViT\n",
    "Switch to `OWL-ViT` to see how it performs with zero-shot learning capabilities. Zero-shot means detecting objects without prior specific training.\n",
    "\n",
    "OWL-ViT (Open Vocabulary Learning with Vision Transformers) is a cutting-edge model designed for open vocabulary object detection. Unlike traditional models, OWL-ViT combines vision transformers with text embeddings, enabling it to:\\n\\n\n",
    "- Understand textual descriptions of objects, even if it hasn't seen them during training.\n",
    "- Detect and classify objects based on descriptive input, making it suitable for diverse applications.\n",
    "- Perform zero-shot learning by generalizing to new object classes without additional training.\\n\\n\"\n",
    "\n",
    "**Steps in Using OWL-ViT:**\n",
    "1. Model Initialization**: Set up the OWL-ViT model.\n",
    "2. Text Input for Object Descriptions: Provide descriptive prompts (e.g., 'a red car' or 'a black cat to guide detection.\n",
    "3. Inference and Visualization: Process an image or video, detect objects based on text descriptions and visualize results.\\n\\n\"\n",
    "\n",
    "OWL-ViT excels in scenarios where predefined object classes are insufficient, such as detecting rare or domain-specific objects.\n",
    "\n",
    "**Support Material**:\n",
    "- https://huggingface.co/docs/transformers/en/model_doc/owlvit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ec801e56774e79a5be3651eb28cf5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/392 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee5df6fad9c4f338667ec101c21bf85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/775 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73712bc2dca495bba310fc6e196fe08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7f0941fa234721a18d4b690c8c1382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3595aa01f04959b94d762ebaeaa847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862659c19f6a471981cdb9f74520d08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dabde2b18a8b46e88df0386c66dc8f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/613M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected a church  with confidence 0.116 at location [706.68, 35.65, 870.62, 342.14]\n",
      "Detected a person on the floor with confidence 0.119 at location [1265.73, 347.33, 1309.63, 423.22]\n",
      "Detected a person on the floor with confidence 0.144 at location [729.04, 301.7, 873.73, 509.65]\n",
      "Detected a person on the floor with confidence 0.109 at location [932.93, 316.24, 1046.45, 524.26]\n",
      "Detected a person on the floor with confidence 0.144 at location [1084.81, 326.57, 1179.79, 470.67]\n",
      "Detected a person on the floor with confidence 0.11 at location [1295.77, 343.81, 1340.44, 421.08]\n",
      "Detected a person on the floor with confidence 0.139 at location [1337.02, 344.97, 1389.21, 425.86]\n",
      "Detected a person on the floor with confidence 0.14 at location [1558.68, 294.65, 1722.22, 665.15]\n",
      "Detected a person on the floor with confidence 0.127 at location [1264.44, 347.31, 1313.74, 428.22]\n",
      "Detected a person on the floor with confidence 0.13 at location [1418.98, 349.44, 1474.32, 440.56]\n",
      "Detected a person on the floor with confidence 0.156 at location [1040.65, 408.02, 1329.82, 718.58]\n",
      "Detected a person on the floor with confidence 0.161 at location [1165.25, 425.62, 1484.56, 841.4]\n",
      "Detected a person on the floor with confidence 0.194 at location [408.87, 595.92, 658.65, 923.15]\n",
      "Detected a person on the floor with confidence 0.333 at location [584.38, 691.08, 1154.21, 1000.75]\n",
      "[706.68, 35.65, 870.62, 342.14]\n",
      "[1265.73, 347.33, 1309.63, 423.22]\n",
      "[729.04, 301.7, 873.73, 509.65]\n",
      "[932.93, 316.24, 1046.45, 524.26]\n",
      "[1084.81, 326.57, 1179.79, 470.67]\n",
      "[1295.77, 343.81, 1340.44, 421.08]\n",
      "[1337.02, 344.97, 1389.21, 425.86]\n",
      "[1558.68, 294.65, 1722.22, 665.15]\n",
      "[1264.44, 347.31, 1313.74, 428.22]\n",
      "[1418.98, 349.44, 1474.32, 440.56]\n",
      "[1040.65, 408.02, 1329.82, 718.58]\n",
      "[1165.25, 425.62, 1484.56, 841.4]\n",
      "[408.87, 595.92, 658.65, 923.15]\n",
      "[584.38, 691.08, 1154.21, 1000.75]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "\n",
    "image = Image.open(\"images/street_scene.jpg\")\n",
    "\n",
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "\n",
    "\n",
    "text_labels = [[\"a person on the floor\", \"a church \"]]\n",
    "\n",
    "inputs = processor(text=text_labels, images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n",
    "target_sizes = torch.tensor([(image.height, image.width)])\n",
    "\n",
    "# Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n",
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs=outputs, target_sizes=target_sizes, threshold=0.1, text_labels=text_labels\n",
    ")\n",
    "# Retrieve predictions for the first image for the corresponding text queries\n",
    "result = results[0]\n",
    "boxes, scores, text_labels = result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]\n",
    "\n",
    "for box, score, text_label in zip(boxes, scores, text_labels):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(f\"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}\")\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    print(box)\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2)\n",
    "    )\n",
    "\n",
    "\n",
    "def show_boxes_and_labels_on_image(raw_image, boxes, labels, scores):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(raw_image)\n",
    "    for i, box in enumerate(boxes):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        show_box(box, plt.gca())\n",
    "        plt.text(\n",
    "            x=box[0],\n",
    "            y=box[1] - 12,\n",
    "            s=f\"{labels[i]}: {scores[i]:,.4f}\",\n",
    "            c=\"beige\",\n",
    "            path_effects=[pe.withStroke(linewidth=4, foreground=\"darkgreen\")],\n",
    "        )\n",
    "    plt.axis(\"on\")\n",
    "    plt.show()\n",
    "    #lt.savefig(\"streetscene_with_detections.jpg\")\n",
    "\n",
    "\n",
    "\n",
    "# Show the image with the bounding boxes\n",
    "show_boxes_and_labels_on_image(\n",
    "    image,\n",
    "    boxes,\n",
    "    text_labels,\n",
    "    scores\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
