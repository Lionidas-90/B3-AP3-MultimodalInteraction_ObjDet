{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. OpenAI VLM (GPT) - Basics\n",
    "This section demonstrates the basic usage of OpenAI's Vision Language Model (VLM) capabilities using GPT-4.1.\n",
    "We will use the OpenAI API to analyze an image and provide detailed textual insights.\n",
    "\n",
    "**Support Material**\n",
    "\n",
    "- https://platform.openai.com/docs/quickstart\n",
    "- https://platform.openai.com/docs/guides/text\n",
    "- https://platform.openai.com/docs/guides/images-vision?api-mode=chat\n",
    "- https://platform.openai.com/docs/guides/structured-outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv  \n",
    "import base64\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "openAIclient = openai.OpenAI()\n",
    "\n",
    "\n",
    "# Path to your image\n",
    "img = \"images/street_scene.jpg\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image depicts a lively urban street scene at a crosswalk in a city. In the foreground, there is a bench with three\n",
      "people: an older man in a suit sitting with a thoughtful expression, a woman in a red top reading a newspaper, and a\n",
      "young woman standing nearby looking at her phone.   On the sidewalk next to the bench, a young boy is sitting cross-\n",
      "legged, engrossed in a tablet or phone, while a person wearing a red hoodie and jeans lies on the ground near him.\n",
      "Several pigeons are scattered around them.  In the middle of the street on the crosswalk, a person on a motorcycle\n",
      "wearing a helmet is riding, a man is walking while playing a guitar, and another person is on a scooter. Cars are moving\n",
      "through the intersection, and the traffic light above shows yellow.   The background includes tall modern buildings, an\n",
      "older church-like building, and several pedestrians walking along the sidewalks. The overall atmosphere is a blend of\n",
      "calm and activity in a typical city environment.\n"
     ]
    }
   ],
   "source": [
    "#basic call to gpt with prompt and image\n",
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "\n",
    "response = str(completion.choices[0].message.content)\n",
    "print(textwrap.fill(response, width=120))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1.1 Structured Output\n",
    "Here, we expand upon the VLM example to request structured outputs. This approach allows for extracting \n",
    "well-organized information from images in a machine-readable format, such as JSON.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/structured-outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe the image in detail\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ]}\n",
    "    ],\n",
    "    response_format={ \"type\": \"json_object\" },# NEW!!\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "returnValue = completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parse the json in a dict structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['scene', 'time_of_day', 'foreground', 'midground', 'background', 'traffic_light', 'overall_mood'])\n"
     ]
    }
   ],
   "source": [
    "output = json.loads(returnValue)\n",
    "#json. loads() converts JSON strings to Python objects\n",
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can access specific infos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urban city street with a mix of modern and older architecture\n"
     ]
    }
   ],
   "source": [
    "print(output[\"scene\"])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# JSON Schema for Controlled Structured Outputs\n",
    "In this section, we define a JSON schema for a more controlled and specific output from the model. \n",
    "Using this schema, we can ensure the model adheres to predefined data types and structures while describing images.In this case we will provide the json schema directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe the image in detail\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ]}\n",
    "    ],\n",
    "    response_format={\n",
    "                \"type\": \"json_schema\",    \n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"img_extract\",\n",
    "                    \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"numberOfPeople\": {\n",
    "                        \"type\":\"integer\",\n",
    "                        \"description\": \"The total number of people in the environment\",\n",
    "                        \"minimum\": 0\n",
    "                        },\n",
    "                        \"atmosphere\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
    "                        },\n",
    "                        \"hourOfTheDay\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The hour of the day in 24-hour format\",\n",
    "                        \"minimum\": 0,\n",
    "                        \"maximum\": 23\n",
    "                        },\n",
    "                        \"people\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"List of people and their details\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                            \"position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Position of the person in the environment, e.g., standing, sitting, etc.\"\n",
    "                            },\n",
    "                            \"age\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Age of the person\",\n",
    "                                \"minimum\": 0\n",
    "                            },\n",
    "                            \"activity\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Activity the person is engaged in, e.g., reading, talking, etc.\"\n",
    "                            },\n",
    "                            \"gender\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Gender of the person\",\n",
    "                                \"enum\": [\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"]\n",
    "                            }\n",
    "                            },\n",
    "                            \"required\": [\"position\", \"age\", \"activity\", \"gender\"]\n",
    "                        }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"numberOfPeople\", \"atmosphere\", \"hourOfTheDay\", \"people\"]\n",
    "                    }}},\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "returnValue = completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image_extraction = json.loads(returnValue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'position': 'sitting on the sidewalk',\n",
       "  'age': 16,\n",
       "  'activity': 'using a smartphone',\n",
       "  'gender': 'male'},\n",
       " {'position': 'lying on the sidewalk',\n",
       "  'age': 18,\n",
       "  'activity': 'resting or sleeping',\n",
       "  'gender': 'male'},\n",
       " {'position': 'sitting on a bench',\n",
       "  'age': 65,\n",
       "  'activity': 'reading a newspaper',\n",
       "  'gender': 'female'},\n",
       " {'position': 'sitting on a bench',\n",
       "  'age': 70,\n",
       "  'activity': 'thinking or resting with hand on face',\n",
       "  'gender': 'male'},\n",
       " {'position': 'walking on the sidewalk',\n",
       "  'age': 20,\n",
       "  'activity': 'using a smartphone',\n",
       "  'gender': 'female'},\n",
       " {'position': 'walking on the street',\n",
       "  'age': 30,\n",
       "  'activity': 'playing guitar',\n",
       "  'gender': 'male'},\n",
       " {'position': 'riding a motorcycle',\n",
       "  'age': 35,\n",
       "  'activity': 'driving',\n",
       "  'gender': 'male'},\n",
       " {'position': 'riding a scooter',\n",
       "  'age': 28,\n",
       "  'activity': 'driving',\n",
       "  'gender': 'female'},\n",
       " {'position': 'walking on the sidewalk',\n",
       "  'age': 25,\n",
       "  'activity': 'walking',\n",
       "  'gender': 'female'},\n",
       " {'position': 'walking on the sidewalk',\n",
       "  'age': 22,\n",
       "  'activity': 'walking',\n",
       "  'gender': 'female'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_image_extraction[\"people\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively: \n",
    "\n",
    "\n",
    "OpenAI SDKs for Python and JavaScript also make it easy to define object schemas using Pydantic and Zod respectively. Below, you can see how to extract information from unstructured text that conforms to a schema defined in code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    position: str \n",
    "    age: int \n",
    "    activity: str \n",
    "    gender: str\n",
    "\n",
    "\n",
    "class ImageExtraction(BaseModel):\n",
    "    number_of_people: int \n",
    "    atmosphere: str \n",
    "    hour_of_the_day: int \n",
    "    people: list[Person] \n",
    "\n",
    "completion = openAIclient.beta.chat.completions.parse(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
    "        {\"role\": \"user\", \"content\": \"describe the image in detail\"}\n",
    "    ],\n",
    "    response_format=ImageExtraction,\n",
    ")\n",
    "\n",
    "output_image_extraction = completion.choices[0].message.parsed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then integrate the extracted information in full or partially in a new prompt for a new extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alert service prompt \n",
    "\n",
    "alert_sys_prompt = \" you are an experienced first aid paramedical\"\n",
    "alert_prompt= \"\"\"Extract from the following scene analysis give to you in json format, \n",
    "if anyone might be in danger and if the Child Hospital or normal Hospital should be alerted. \n",
    "Give the a concise answer\n",
    "The situation is given to you from this object: \"\"\" + str(output_image_extraction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No one appears to be in immediate danger. The 18-year-old lying on the sidewalk seems to be resting or sleeping but is\n",
      "not clearly in distress. No hospital alert is necessary at this time.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": alert_prompt},\n",
    "        {\"role\": \"user\", \"content\": alert_prompt}\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "\n",
    "response = str(completion.choices[0].message.content)\n",
    "print(textwrap.fill(response, width=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The youngest person in the list is the 16-year-old male who is sitting on the sidewalk and using a smartphone.  Please\n",
      "provide the image so I can identify the coordinates of this person in the image and give you the box_2d normalized to\n",
      "0-1000.\n"
     ]
    }
   ],
   "source": [
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Considering this list of people\"+str(output_image_extraction[\"people\"])+\".Identify the youngest in the picture I provide and give me back their coordinates. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000.\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "\n",
    "response = str(completion.choices[0].message.content)\n",
    "print(textwrap.fill(response, width=120))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Google VLM (Gemini)\n",
    "This section demonstrates the use of Google's Vision Language Model, Gemini. \n",
    "We explore basic text generation as well as its ability to analyze images and provide relevant outputs.\n",
    "\n",
    "**Support Material**:\n",
    "- https://ai.google.dev/gemini-api/docs/quickstart\n",
    "- https://ai.google.dev/gemini-api/docs/text-generation\n",
    "- https://ai.google.dev/gemini-api/docs/image-understanding\n",
    "- https://ai.google.dev/gemini-api/docs/structured-output?example=recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from dotenv import load_dotenv  \n",
    "from google import genai\n",
    "from PIL import Image\n",
    "import textwrap\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "client = genai.Client()\n",
    "\n",
    "# Path to your image\n",
    "img = \"images/street_scene.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's like giving a computer a \"brain.\" It learns by looking at many examples, then it can recognize things, understand\n",
      "your voice, or help you with tasks. Think of it as a very clever assistant.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=\"Explain how AI works to a 90 years old. in few words\"\n",
    ")\n",
    "\n",
    "print(textwrap.fill(response.text, width=120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and with images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This vibrant and bustling city street scene captures a dynamic moment, bathed in what appears to be the warm, golden\n",
      "light of either early morning or late afternoon. The perspective places the viewer slightly above the street level,\n",
      "looking down a wide thoroughfare lined with buildings of varying architectural styles.  In the immediate **foreground**,\n",
      "a prominent zebra crosswalk spans the width of the street. On the left side of the crosswalk, a young person sits cross-\n",
      "legged on the pavement, engrossed in a tablet or smartphone. Next to them is a rustic wooden pot overflowing with bright\n",
      "red flowers, possibly geraniums. Nearby, several pigeons peck at the ground.  Moving right, and perhaps the most\n",
      "striking element, a young man in a red hoodie and blue jeans lies completely prone on the paved sidewalk, seemingly\n",
      "unconscious or asleep. His presence creates a stark contrast with the surrounding activity, as other pedestrians walk\n",
      "past, appearing largely oblivious. More pigeons are scattered around him.  Further to the right, a classic wooden park\n",
      "bench is occupied by two individuals. An older man, dressed in a dark suit with glasses, sits thoughtfully, his hand\n",
      "resting on his chin, gazing into the distance. Next to him, a woman with blonde hair, wearing a striped red and white\n",
      "long-sleeved shirt and blue jeans, is absorbed in reading a newspaper or magazine.  Just beyond the bench, a young woman\n",
      "with long dark hair, wearing a pink t-shirt, denim shorts, and white sneakers, walks away from the viewer, holding a\n",
      "phone or small object in her hand.  The **midground** is a flurry of motion. Vehicles are depicted with a strong sense\n",
      "of speed through motion blur. A silver sedan, possibly a taxi with a blurred sign on its roof, streaks across the\n",
      "crosswalk. Behind it, a silver SUV and a golden-tan sedan also move rapidly. A motorcyclist in dark gear and a helmet\n",
      "rides across the crosswalk from left to right, his motorcycle a blur of motion. Ahead of him, a man in dark clothes\n",
      "walks with a guitar, strumming as he moves, appearing to be a street performer. Further back, a woman on a white scooter\n",
      "or moped also navigates the street.  Overhead, a three-light traffic signal, showing yellow lights, hangs suspended over\n",
      "the intersection.  The **background** reveals a classic urban landscape. On the left, multi-story brick buildings with\n",
      "numerous windows and green awnings over ground-level storefronts stretch into the distance. On the right, more multi-\n",
      "story buildings, some with brick facades and others appearing more modern with glass and concrete, line the street. A\n",
      "particular storefront on the right side has a sign that reads \"BILLY'S GOOD CHEAP.\" In the middle distance, an older,\n",
      "possibly gothic-style church building with a prominent spire stands out amongst taller, modern skyscrapers that dominate\n",
      "the skyline. The low sun casts a warm, golden glow across the scene, highlighting the buildings and creating a sense of\n",
      "depth and atmosphere.  Overall, the scene is a rich tapestry of urban life, combining moments of quiet contemplation\n",
      "with the constant rush of city traffic and diverse human interactions, all bathed in an inviting light.\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(img)\n",
    "\n",
    "response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
    "                                          contents=[im, \"Describe the scene in details\\n\"],\n",
    "                                          )\n",
    "\n",
    "print(textwrap.fill(response.text, width=120))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also here we can extract structured output (Gemini actually prefers pydantic syntax - let's see what happens with a schema as before)-> check limitations in https://ai.google.dev/gemini-api/docs/structured-output?example=recipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"label\": \"car\",\n",
      "    \"box_2d\": [0, 311, 327, 567],\n",
      "    \"color\": \"silver\",\n",
      "    \"material\": \"metal\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"driving\",\n",
      "    \"rot_yaw\": -20.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"car\",\n",
      "    \"box_2d\": [266, 335, 514, 506],\n",
      "    \"color\": \"orange\",\n",
      "    \"material\": \"metal\",\n",
      "    \"occluded\": true,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"driving\",\n",
      "    \"rot_yaw\": -30.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"car\",\n",
      "    \"box_2d\": [438, 339, 597, 411],\n",
      "    \"color\": \"silver\",\n",
      "    \"material\": \"metal\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"driving\",\n",
      "    \"rot_yaw\": -10.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [317, 301, 477, 451],\n",
      "    \"color\": \"black\",\n",
      "    \"material\": \"leather\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"riding motorcycle\",\n",
      "    \"rot_yaw\": -15.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"motorcycle\",\n",
      "    \"box_2d\": [329, 396, 488, 569],\n",
      "    \"color\": \"silver\",\n",
      "    \"material\": \"metal\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"moving\",\n",
      "    \"rot_yaw\": -15.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [580, 310, 679, 440],\n",
      "    \"color\": \"black\",\n",
      "    \"material\": \"fabric\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"riding scooter\",\n",
      "    \"rot_yaw\": -45.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"scooter\",\n",
      "    \"box_2d\": [588, 390, 706, 502],\n",
      "    \"color\": \"silver\",\n",
      "    \"material\": \"metal\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"moving\",\n",
      "    \"rot_yaw\": -45.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [321, 701, 621, 959],\n",
      "    \"color\": \"red\",\n",
      "    \"material\": \"fabric\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"lying down\",\n",
      "    \"rot_yaw\": 90.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [209, 610, 396, 871],\n",
      "    \"color\": \"green\",\n",
      "    \"material\": \"fabric\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"sitting on ground\",\n",
      "    \"rot_yaw\": 10.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"tablet\",\n",
      "    \"box_2d\": [284, 755, 319, 792],\n",
      "    \"color\": \"black\",\n",
      "    \"material\": \"plastic\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"held\",\n",
      "    \"rot_yaw\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [490, 311, 630, 521],\n",
      "    \"color\": \"black\",\n",
      "    \"material\": \"fabric\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"walking, playing guitar\",\n",
      "    \"rot_yaw\": -25.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"guitar\",\n",
      "    \"box_2d\": [510, 354, 612, 484],\n",
      "    \"color\": \"brown\",\n",
      "    \"material\": \"wood\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"held\",\n",
      "    \"rot_yaw\": -25.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [560, 480, 729, 737],\n",
      "    \"color\": \"grey\",\n",
      "    \"material\": \"fabric\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"sitting on bench\",\n",
      "    \"rot_yaw\": -45.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [630, 479, 836, 820],\n",
      "    \"color\": \"red\",\n",
      "    \"material\": \"fabric\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"sitting on bench, reading\",\n",
      "    \"rot_yaw\": -45.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"newspaper\",\n",
      "    \"box_2d\": [694, 620, 736, 688],\n",
      "    \"color\": \"white\",\n",
      "    \"material\": \"paper\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"held\",\n",
      "    \"rot_yaw\": -45.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [862, 270, 994, 499],\n",
      "    \"color\": \"pink\",\n",
      "    \"material\": \"fabric\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"walking\",\n",
      "    \"rot_yaw\": -70.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"cup\",\n",
      "    \"box_2d\": [930, 390, 951, 420],\n",
      "    \"color\": \"white\",\n",
      "    \"material\": \"plastic\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"held\",\n",
      "    \"rot_yaw\": -70.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"bench\",\n",
      "    \"box_2d\": [637, 638, 999, 960],\n",
      "    \"color\": \"brown\",\n",
      "    \"material\": \"wood\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"stationary\",\n",
      "    \"rot_yaw\": -45.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"flower pot\",\n",
      "    \"box_2d\": [88, 762, 271, 921],\n",
      "    \"color\": \"brown\",\n",
      "    \"material\": \"wood\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"stationary\",\n",
      "    \"rot_yaw\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"pigeon\",\n",
      "    \"box_2d\": [618, 819, 709, 902],\n",
      "    \"color\": \"grey\",\n",
      "    \"material\": \"feather\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"standing\",\n",
      "    \"rot_yaw\": -10.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"pigeon\",\n",
      "    \"box_2d\": [493, 850, 560, 911],\n",
      "    \"color\": \"grey\",\n",
      "    \"material\": \"feather\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"standing\",\n",
      "    \"rot_yaw\": 15.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"pigeon\",\n",
      "    \"box_2d\": [500, 591, 552, 631],\n",
      "    \"color\": \"grey\",\n",
      "    \"material\": \"feather\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"standing\",\n",
      "    \"rot_yaw\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"pigeon\",\n",
      "    \"box_2d\": [340, 680, 391, 720],\n",
      "    \"color\": \"grey\",\n",
      "    \"material\": \"feather\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"standing\",\n",
      "    \"rot_yaw\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"crosswalk\",\n",
      "    \"box_2d\": [0, 400, 1000, 800],\n",
      "    \"color\": \"white\",\n",
      "    \"material\": \"asphalt\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"flat\",\n",
      "    \"rot_yaw\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"traffic light\",\n",
      "    \"box_2d\": [632, 2, 702, 142],\n",
      "    \"color\": \"yellow\",\n",
      "    \"material\": \"metal\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"hanging\",\n",
      "    \"rot_yaw\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"building\",\n",
      "    \"box_2d\": [0, 0, 452, 399],\n",
      "    \"color\": \"brown\",\n",
      "    \"material\": \"brick\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"stationary\",\n",
      "    \"rot_yaw\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"building\",\n",
      "    \"box_2d\": [400, 0, 750, 350],\n",
      "    \"color\": \"grey\",\n",
      "    \"material\": \"glass\",\n",
      "    \"occluded\": true,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"stationary\",\n",
      "    \"rot_yaw\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"church\",\n",
      "    \"box_2d\": [392, 99, 502, 353],\n",
      "    \"color\": \"grey\",\n",
      "    \"material\": \"stone\",\n",
      "    \"occluded\": true,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"stationary\",\n",
      "    \"rot_yaw\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"building\",\n",
      "    \"box_2d\": [751, 99, 1000, 453],\n",
      "    \"color\": \"brown\",\n",
      "    \"material\": \"brick\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"stationary\",\n",
      "    \"rot_yaw\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"street light\",\n",
      "    \"box_2d\": [50, 0, 100, 320],\n",
      "    \"color\": \"grey\",\n",
      "    \"material\": \"metal\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"standing\",\n",
      "    \"rot_yaw\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"street light\",\n",
      "    \"box_2d\": [630, 100, 660, 270],\n",
      "    \"color\": \"grey\",\n",
      "    \"material\": \"metal\",\n",
      "    \"occluded\": false,\n",
      "    \"is_3d\": true,\n",
      "    \"pose\": \"standing\",\n",
      "    \"rot_yaw\": 0.0\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "json_schema = {\n",
    "                    \"name\": \"img_extract\",\n",
    "                    \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"numberOfPeople\": {\n",
    "                        \"type\":\"integer\",\n",
    "                        \"description\": \"The total number of people in the environment\",\n",
    "                        \"minimum\": 0\n",
    "                        },\n",
    "                        \"atmosphere\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
    "                        },\n",
    "                        \"hourOfTheDay\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The hour of the day in 24-hour format\",\n",
    "                        \"minimum\": 0,\n",
    "                        \"maximum\": 23\n",
    "                        },\n",
    "                        \"people\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"List of people and their details\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                            \"position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Position of the person in the environment, e.g., standing, sitting, etc.\"\n",
    "                            },\n",
    "                            \"age\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Age of the person\",\n",
    "                                \"minimum\": 0\n",
    "                            },\n",
    "                            \"activity\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Activity the person is engaged in, e.g., reading, talking, etc.\"\n",
    "                            },\n",
    "                            \"gender\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Gender of the person\",\n",
    "                                \"enum\": [\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"]\n",
    "                            }\n",
    "                            },\n",
    "                            \"required\": [\"position\", \"age\", \"activity\", \"gender\"]\n",
    "                        }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"numberOfPeople\", \"atmosphere\", \"hourOfTheDay\", \"people\"]}}\n",
    "\n",
    "\n",
    "\n",
    "config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_json_schema\": json_schema,\n",
    "    }\n",
    "\n",
    "\n",
    "response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
    "                                          contents=[im, \"Describe the scene in details, follwoing exactly the given json schema\\n\"],\n",
    "                                          config=config\n",
    "                                          )\n",
    "\n",
    "\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it match your schema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use Gemini to detect an object in the image and get its coordinates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'box_2d': [336, 686, 650, 814]}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Identify the youngest in the picture and give me back their coordinates. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000.\"\n",
    "\n",
    "\n",
    "config={\"response_mime_type\": \"application/json\"}\n",
    "\n",
    "response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
    "                                          contents=[img, prompt],\n",
    "                                          config=config\n",
    "                                          )\n",
    "\n",
    "bounding_boxes = json.loads(response.text)\n",
    "print(bounding_boxes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini2+ was trained specifically for object detection/ segmentation tasks. More details: https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Extract Structured Infos from Hand-written note - GPT & Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try **not** to extract structured information from a handwritten note (e.g., `prescription1.jpg`) using **both models**.\n",
    "\n",
    "Consider the file: `/images/prescription1.jpg`.  \n",
    "Have a look at it.\n",
    "\n",
    "### JSON Schema\n",
    "Let’s define a JSON schema for the extraction task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema_prescription = {\n",
    " \"name\": \"prescription_extract\",\n",
    "\"schema\": {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"doctor_name\": { \"type\": \"string\" },\n",
    "    \"patient_name\": { \"type\": \"string\" },\n",
    "    \"patient_dob\": { \"type\": \"string\" },\n",
    "    \"meds\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"name\": { \"type\": \"string\" },\n",
    "          \"dose\": { \"type\": \"string\" },\n",
    "          \"frequency\": { \"type\": \"string\" },\n",
    "          \"instructions\": { \"type\": \"string\" }\n",
    "        },\n",
    "        \"required\": [\"name\"]\n",
    "      }\n",
    "    },\n",
    "    \"signature\": { \"type\": \"boolean\" }\n",
    "  },\n",
    "  \"required\": [\"doctor_name\", \"patient_name\", \"meds\"]\n",
    "}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract structured infos using Gemini: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"doctor_name\": \"Dr. Markus Hütter\",\n",
      "  \"doctor_profession\": \"Arzt\",\n",
      "  \"patient_name\": \"Claudie Fischer\",\n",
      "  \"patient_dob\": \"1.4.1978\",\n",
      "  \"patient_gender\": \"f\",\n",
      "  \"medications\": [\n",
      "    \"name\",\n",
      "    \"Ibuprofen\",\n",
      "    \"dosage\",\n",
      "    \"400mg\",\n",
      "    \"frequency\",\n",
      "    \"3x\",\n",
      "    \"instructions\",\n",
      "    \"nach dem Essen\"\n",
      "  ],\n",
      "  \"signature\": \"present\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(\"images/prescription1.jpg\")\n",
    "\n",
    "config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_json_schema\": json_schema_prescription,\n",
    "    }\n",
    "\n",
    "\n",
    "response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
    "                                          contents=[im, \"Extract infos from image, follwoing the given json schema.\\n\"],\n",
    "                                          config=config\n",
    "                                          )\n",
    "\n",
    "\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the output is **not valid JSON** and contains extra strings, it must be **parsed** before it can be loaded into a Python dict.  \n",
    "Below is an example helper function that does this.\n",
    "\n",
    "> **Note:** Since Gemini returns a Pydantic model, you *could* use Pydantic methods to handle parsing.  \n",
    "> We avoid that here to keep the workflow generally compatible across models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json \n",
    "def parse_json_in_output(output):\n",
    "    \"\"\"\n",
    "    Extracts and converts JSON-like data from the given text output to a Python dictionary.\n",
    "    \n",
    "    Args:\n",
    "        output (str): The text output containing the JSON data.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The parsed JSON data as a Python dictionary.\n",
    "    \"\"\"\n",
    "    # Regex to extract JSON-like portion\n",
    "    json_match = re.search(r\"\\{.*?\\}\", output, re.DOTALL)\n",
    "    if json_match:\n",
    "        json_str = json_match.group(0)\n",
    "        # Fix single quotes and ensure proper JSON formatting\n",
    "        json_str = json_str.replace(\"'\", '\"')  # Replace single quotes with double quotes\n",
    "        try:\n",
    "            # Convert the fixed JSON string into a dictionary\n",
    "            json_data = json.loads(json_str)\n",
    "            return json_data\n",
    "        except json.JSONDecodeError:\n",
    "            return \"The extracted JSON is still not valid after formatting.\"\n",
    "    else:\n",
    "        return \"No JSON data found in the given output.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doctor_name': 'Dr. Markus Hütter', 'doctor_profession': 'Arzt', 'patient_name': 'Claudie Fischer', 'patient_dob': '1.4.1978', 'patient_gender': 'f', 'medications': ['name', 'Ibuprofen', 'dosage', '400mg', 'frequency', '3x', 'instructions', 'nach dem Essen'], 'signature': 'present'}\n"
     ]
    }
   ],
   "source": [
    "print(parse_json_in_output(response.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doctor_name': 'Dr. Markus Hütter',\n",
       " 'doctor_profession': 'Arzt',\n",
       " 'patient_name': 'Claudie Fischer',\n",
       " 'patient_dob': '1.4.1978',\n",
       " 'patient_gender': 'f',\n",
       " 'medications': ['name',\n",
       "  'Ibuprofen',\n",
       "  'dosage',\n",
       "  '400mg',\n",
       "  'frequency',\n",
       "  '3x',\n",
       "  'instructions',\n",
       "  'nach dem Essen'],\n",
       " 'signature': 'present'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same with GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = \"images/prescription1.jpg\"\n",
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe the image in detail\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(im)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ]}\n",
    "    ],\n",
    "    response_format={\n",
    "                \"type\": \"json_schema\",   \"json_schema\": json_schema_prescription},\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "returnValue = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"doctor_name\":\"Dr. Markus Müller\",\"patient_name\":\"Claudia Fischer\",\"patient_dob\":\"1.4.1978\",\"meds\":[{\"name\":\"Ibuprofen\",\"dose\":\"400 mg\",\"frequency\":\"3x\",\"instructions\":\"nach dem Essen\"}],\"signature\":true}'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returnValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any difference wiht the output of Gemini vs your schema? \n",
    "\n",
    "No need for parsing now. We load the json in a python dict structure with json.loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doctor_name': 'Dr. Markus Müller', 'patient_name': 'Claudia Fischer', 'patient_dob': '1.4.1978', 'meds': [{'name': 'Ibuprofen', 'dose': '400 mg', 'frequency': '3x', 'instructions': 'nach dem Essen'}], 'signature': True}\n"
     ]
    }
   ],
   "source": [
    "print(json.loads(returnValue))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
